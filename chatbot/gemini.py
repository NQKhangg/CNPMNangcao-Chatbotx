# pip install google-genai faiss-cpu sentence-transformers fastapi uvicorn
import os
import re
import json
import pickle
from datetime import datetime
import numpy as np
import faiss
from google import genai
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from fastapi.middleware.cors import CORSMiddleware

# --- C·∫§U H√åNH ·ª®NG D·ª§NG ---

# Kh·ªüi t·∫°o FastAPI
app = FastAPI()

# C·∫•u h√¨nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# C·∫•u h√¨nh Gemini Client
# terminal : export GEMINI_API_KEY=...
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("GEMINI_API_KEY environment variable is not set")
client = genai.Client(api_key=api_key)

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n l∆∞u Cache
CACHE_DIR = "./cache"
INDEX_FILE = os.path.join(CACHE_DIR, "faiss.index")
DOCS_FILE = os.path.join(CACHE_DIR, "documents.pkl")

# Bi·∫øn to√†n c·ª•c l∆∞u tr·∫°ng th√°i RAG
rag_db = {
    "documents": [],  # Danh s√°ch vƒÉn b·∫£n g·ªëc
    "index": None,    # Ch·ªâ m·ª•c t√¨m ki·∫øm FAISS
    "model": None     # Model Embedding
}

# --- C√ÅC H√ÄM TI·ªÜN √çCH (HELPER FUNCTIONS) ---

def clean_html(raw_html):
    """Lo·∫°i b·ªè c√°c th·∫ª HTML kh·ªèi chu·ªói vƒÉn b·∫£n."""
    if not raw_html:
        return ""
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext.strip()

def format_price(value):
    """ƒê·ªãnh d·∫°ng gi√° ti·ªÅn sang ki·ªÉu Vi·ªát Nam (VD: 100.000 ƒë)."""
    return "{:,.0f} ƒë".format(value).replace(",", ".")

def parse_mongo_date(date_obj):
    """Chuy·ªÉn ƒë·ªïi ƒë·ªëi t∆∞·ª£ng ng√†y th√°ng t·ª´ MongoDB sang chu·ªói dd/mm/yyyy."""
    if not date_obj:
        return "Kh√¥ng th·ªùi h·∫°n"
    if isinstance(date_obj, str):
        return date_obj
    # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng MongoDB {$date: "..."}
    if isinstance(date_obj, dict) and '$date' in date_obj:
        try:
            # C·∫Øt b·ªè ph·∫ßn mili gi√¢y n·∫øu c·∫ßn ho·∫∑c parse ISO
            dt_str = date_obj['$date'].replace('Z', '+00:00')
            dt = datetime.fromisoformat(dt_str)
            return dt.strftime("%d/%m/%Y")
        except Exception:
            return str(date_obj['$date'])
    return str(date_obj)

# --- H√ÄM X·ª¨ L√ù RAG CH√çNH ---

def load_and_index_data(force_refresh=False):
    """
    T·∫£i d·ªØ li·ªáu, t·∫°o embedding v√† index FAISS.
    Args:
        force_refresh (bool): N·∫øu True, x√≥a cache c≈© v√† t·∫°o l·∫°i t·ª´ ƒë·∫ßu.
    """
    
    # 0. Load Model Embedding
    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    print(f"ƒêang t·∫£i model embedding: {model_name}...")
    try:
        model = SentenceTransformer(model_name)
        rag_db["model"] = model
    except Exception as e:
        print(f"L·ªói t·∫£i model embedding: {e}")
        return

    # 1. Ki·ªÉm tra Cache
    if not force_refresh and os.path.exists(INDEX_FILE) and os.path.exists(DOCS_FILE):
        print("ƒêang t·∫£i d·ªØ li·ªáu t·ª´ cache ./cache...")
        try:
            # Load Index FAISS
            index = faiss.read_index(INDEX_FILE)
            
            # Load Documents
            with open(DOCS_FILE, "rb") as f:
                documents = pickle.load(f)
            
            rag_db["index"] = index
            rag_db["documents"] = documents
            print(f"ƒê√£ kh√¥i ph·ª•c {len(documents)} t√†i li·ªáu t·ª´ Cache. S·∫µn s√†ng!")
            return 
        except Exception as e:
            print(f"L·ªói ƒë·ªçc cache ({e}). S·∫Ω ti·∫øn h√†nh t·∫°o l·∫°i d·ªØ li·ªáu m·ªõi...")

    # 2. N·∫°p v√† x·ª≠ l√Ω d·ªØ li·ªáu g·ªëc
    print("ƒêang n·∫°p v√† x·ª≠ l√Ω d·ªØ li·ªáu g·ªëc t·ª´ JSON...")
    
    # T·∫°o th∆∞ m·ª•c cache n·∫øu ch∆∞a c√≥
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)

    documents = []
    
    # ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n file
    files = {
        "products": ["../database/data/database.products.json"],
        "blogs": ["../database/data/database.blogs.json"],
        "categories": ["../database/data/database.categories.json"],
        "coupons": ["../database/data/database.coupons.json"]
    }

    def get_valid_path(paths):
        for p in paths:
            if os.path.exists(p): return p
        return None

    # Load Products
    path = get_valid_path(files["products"])
    if path:
        try:
            with open(path, encoding="utf-8") as f:
                products = json.load(f)
                for p in products:
                    if p.get('isDeleted'): continue 

                    nutrition = ", ".join([f"{n.get('label','')}: {n.get('value','')}" for n in p.get('nutrition', [])])
                    tags = ", ".join(p.get('tags', []))
                    desc = clean_html(p.get('description', ''))
                    
                    doc = (
                        f"[S·∫¢N PH·∫®M] {p['name']}\n"
                        f"- Gi√°: {format_price(p.get('price', 0))} (G·ªëc: {format_price(p.get('originalPrice', 0))})\n"
                        f"- ƒê∆°n v·ªã: {p.get('unit', '')}\n"
                        f"- ƒê·∫∑c ƒëi·ªÉm: {p.get('shortDescription', '')}. {desc[:500]}...\n"
                        f"- Dinh d∆∞·ª°ng: {nutrition}\n"
                        f"- B·∫£o qu·∫£n: {p.get('preservation', '')}\n"
                        f"- T·ª´ kh√≥a: {tags}"
                    )
                    documents.append(doc)
            print(f"ƒê√£ load {len(products)} s·∫£n ph·∫©m.")
        except Exception as e:
            print(f"L·ªói ƒë·ªçc Products: {e}")

    # Load Categories
    path = get_valid_path(files["categories"])
    if path:
        try:
            with open(path, encoding="utf-8") as f:
                cats = json.load(f)
                for c in cats:
                    if not c.get('isActive') or c.get('isDeleted'): continue

                    doc = (
                        f"[DANH M·ª§C] {c['name']}\n"
                        f"- M√¥ t·∫£: {c.get('description', '')}"
                    )
                    documents.append(doc)
            print(f"ƒê√£ load {len(cats)} danh m·ª•c.")
        except Exception as e:
            print(f"L·ªói ƒë·ªçc Categories: {e}")

    # Load Blogs
    path = get_valid_path(files["blogs"])
    if path:
        try:
            with open(path, encoding="utf-8") as f:
                blogs = json.load(f)
                for b in blogs:
                    if not b.get('isPublished') or b.get('isDeleted'): continue

                    content_clean = clean_html(b.get('content', ''))
                    tags = ", ".join(b.get('tags', []))
                    
                    doc = (
                        f"[B√ÄI VI·∫æT/M·∫∏O V·∫∂T] {b['title']}\n"
                        f"- Ch·ªß ƒë·ªÅ: {b.get('category', '')}\n"
                        f"- T√≥m t·∫Øt: {b.get('shortDescription', '')}\n"
                        f"- N·ªôi dung ch√≠nh: {content_clean[:800]}...\n"
                        f"- T·ª´ kh√≥a: {tags}"
                    )
                    documents.append(doc)
            print(f"ƒê√£ load {len(blogs)} b√†i vi·∫øt.")
        except Exception as e:
            print(f"L·ªói ƒë·ªçc Blogs: {e}")

    # Load Coupons
    path = get_valid_path(files["coupons"])
    if path:
        try:
            with open(path, encoding="utf-8") as f:
                coupons = json.load(f)
                for c in coupons:
                    if not c.get('isActive') or c.get('isDeleted'): continue
                    
                    if c.get('type') == 'PERCENT':
                        val_str = f"{c.get('value')}%"
                    else:
                        val_str = format_price(c.get('value', 0))

                    expiry = parse_mongo_date(c.get('expiryDate'))
                    
                    limit_info = "Kh√¥ng gi·ªõi h·∫°n" if c.get('usageLimit', 0) == 0 else f"C√≤n {c['usageLimit'] - c.get('usedCount', 0)} l∆∞·ª£t"

                    doc = (
                        f"[M√É GI·∫¢M GI√Å/VOUCHER] M√£: {c['code']}\n"
                        f"- ∆Øu ƒë√£i: Gi·∫£m {val_str}\n"
                        f"- M√¥ t·∫£: {c.get('description', '')}\n"
                        f"- H·∫°n s·ª≠ d·ª•ng: {expiry}\n"
                        f"- T√¨nh tr·∫°ng l∆∞·ª£t d√πng: {limit_info}"
                    )
                    documents.append(doc)
            print(f"ƒê√£ load {len(coupons)} m√£ gi·∫£m gi√°.")
        except Exception as e:
            print(f"L·ªói ƒë·ªçc Coupons: {e}")

    if not documents:
        print("C·∫£nh b√°o: Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c n·∫°p!")
        documents = ["Ch∆∞a c√≥ d·ªØ li·ªáu."]

    # 3. T·∫°o Index FAISS
    print("ƒêang t·∫°o index FAISS...")
    try:
        vectors = model.encode(documents)
        # Chuy·ªÉn ƒë·ªïi sang float32 n·∫øu ch∆∞a ph·∫£i
        vectors = np.array(vectors).astype("float32")
        
        index = faiss.IndexFlatL2(vectors.shape[1])
        index.add(vectors)

        # L∆∞u cache
        print("ƒêang l∆∞u cache...")
        faiss.write_index(index, INDEX_FILE)
        with open(DOCS_FILE, "wb") as f:
            pickle.dump(documents, f)

        # C·∫≠p nh·∫≠t state
        rag_db["documents"] = documents
        rag_db["index"] = index
        print(f"AI Service ƒë√£ s·∫µn s√†ng! T·ªïng c·ªông {len(documents)} t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c index.")
    except Exception as e:
        print(f"L·ªói t·∫°o index: {e}")

def retrieve_context(query, k=3):
    """Truy xu·∫•t c√°c vƒÉn b·∫£n li√™n quan d·ª±a tr√™n query."""
    model = rag_db["model"]
    index = rag_db["index"]
    documents = rag_db["documents"]
    
    if not model or not index:
        return "H·ªá th·ªëng ƒëang kh·ªüi ƒë·ªông..."

    try:
        q_vec = model.encode([query])
        q_vec = np.array(q_vec).astype("float32")
        
        _, idx = index.search(q_vec, k)
        
        results = [documents[i] for i in idx[0] if i < len(documents)]
        return "\n".join(results)
    except Exception as e:
        print(f"L·ªói truy xu·∫•t context: {e}")
        return ""

# --- API ENDPOINTS ---

@app.on_event("startup")
def startup_event():
    load_and_index_data(force_refresh=False)

@app.get("/")
def home():
    return {"status": "AI Service is running", "model": "Gemini 2.5 Flash Lite"}

class ChatRequest(BaseModel):
    question: str

@app.post("/chat")
def chat(request: ChatRequest):
    try:
        user_query = request.question
            
        # 1. T√¨m th√¥ng tin li√™n quan (RAG)
        context = retrieve_context(user_query, k=4)

        # 2. T·∫°o Prompt
        prompt = f"""
            B·∫°n l√† tr·ª£ l√Ω ·∫£o chuy√™n nghi·ªáp c·ªßa c·ª≠a h√†ng th·ª±c ph·∫©m s·∫°ch FreshFood.
            
            D·ªÆ LI·ªÜU T√åM TH·∫§Y T·ª™ C·ª¨A H√ÄNG:
            ---------------------
            {context}
            ---------------------
            
            Y√äU C·∫¶U TR·∫¢ L·ªúI:
            1. D·ª±a CH√çNH X√ÅC v√†o d·ªØ li·ªáu tr√™n ƒë·ªÉ tr·∫£ l·ªùi.
            2. N·∫øu kh√°ch h·ªèi m√≥n ƒÉn, h√£y g·ª£i √Ω m√≥n d·ª±a tr√™n nguy√™n li·ªáu c√≥ trong d·ªØ li·ªáu (v√≠ d·ª•: c√≥ th·ªãt heo -> g·ª£i √Ω th·ªãt kho t√†u).
            3. ƒê·ªëi v·ªõi s·∫£n ph·∫©m, tuy·ªát ƒë·ªëi KH√îNG b·ªãa ƒë·∫∑t gi√° c·∫£ n·∫øu kh√¥ng c√≥ trong d·ªØ li·ªáu.
            4. H√£y t∆∞ v·∫•n nhi·ªát t√¨nh cho kh√°ch h√†ng v·ªÅ t∆∞ v·∫•n b·ªØa ƒÉn, s·ª©c kh·ªèe, ƒë·ªùi s·ªëng, ...
            5. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¢n thi·ªán, s·ª≠ d·ª•ng Emoji ph√π h·ª£p üåøüçé.
            
            C√¢u h·ªèi c·ªßa kh√°ch: {user_query}
        """

        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
        )
        
        return {
            "answer": response.text,
            "context_used": context
        }
    except Exception as e:
        print(f"L·ªói x·ª≠ l√Ω chat: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ƒê·ªÉ ch·∫°y: uvicorn filename:app --reload